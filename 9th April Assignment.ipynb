{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "179cd699-0038-49e5-a071-a68e0d1fcfd6",
   "metadata": {},
   "source": [
    "### Q1. What is Bayes' Theorem?\n",
    "Bayes' theorem is a foundational concept in probability theory and statistics that describes the probability of an event based on prior knowledge of related events. It allows for the updating of probability estimates as new data or evidence is observed, and it is often used in statistical inference and machine learning applications.\n",
    "\n",
    "### Q2. What is the Formula for Bayes' Theorem?\n",
    "The formula for Bayes' theorem is as follows:\n",
    "\\[ P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)} \\]\n",
    "where:\n",
    "- \\( P(A|B) \\) is the conditional probability of event \\( A \\) given that \\( B \\) has occurred.\n",
    "- \\( P(B|A) \\) is the conditional probability of event \\( B \\) given that \\( A \\) has occurred.\n",
    "- \\( P(A) \\) is the prior probability of event \\( A \\).\n",
    "- \\( P(B) \\) is the marginal probability of event \\( B \\).\n",
    "\n",
    "### Q3. How is Bayes' Theorem Used in Practice?\n",
    "Bayes' theorem is used in various domains, including:\n",
    "- **Medical Diagnosis**: To update the probability of a disease given test results.\n",
    "- **Spam Filtering**: To calculate the probability that an email is spam given the words it contains.\n",
    "- **Machine Learning**: In algorithms like Naive Bayes, to classify data based on conditional probabilities.\n",
    "- **Risk Assessment**: To assess the probability of certain risks given historical data or other evidence.\n",
    "\n",
    "### Q4. What is the Relationship Between Bayes' Theorem and Conditional Probability?\n",
    "Bayes' theorem is intrinsically related to conditional probability. It is a way to update the conditional probability of an event based on new evidence. The theorem allows for flipping the conditionals to understand how one event can provide evidence for or against another event.\n",
    "\n",
    "### Q5. How Do You Choose Which Type of Naive Bayes Classifier to Use for Any Given Problem?\n",
    "Choosing the type of Naive Bayes classifier depends on the nature of the data and the distribution of the features:\n",
    "- **Gaussian Naive Bayes**: Use when features are continuous and are assumed to follow a Gaussian (normal) distribution.\n",
    "- **Multinomial Naive Bayes**: Best for discrete features such as word counts or categorical data.\n",
    "- **Bernoulli Naive Bayes**: Ideal for binary/Boolean features or when features are represented by the presence or absence of certain attributes.\n",
    "\n",
    "### Q6. Assignment: Determine the Class for a Given Instance Using Naive Bayes\n",
    "To classify a new instance with features \\( X_1 = 3 \\) and \\( X_2 = 4 \\), let's calculate the conditional probabilities for each class and determine which class has the higher likelihood. Given equal prior probabilities, we'll focus on \\( P(X_1 = 3 \\cap X_2 = 4|A) \\) and \\( P(X_1 = 3 \\cap X_2 = 4|B) \\). Using the table, we can estimate these probabilities.\n",
    "\n",
    "#### Class A\n",
    "- \\( P(X_1 = 3|A) = \\frac{4}{10} \\) (4 out of 10 instances of class A)\n",
    "- \\( P(X_2 = 4|A) = \\frac{3}{10} \\) (3 out of 10 instances of class A)\n",
    "- Assuming independence, \\( P(X_1 = 3 \\cap X_2 = 4|A) = \\frac{4}{10} \\times \\frac{3}{10} = 0.12 \\)\n",
    "\n",
    "#### Class B\n",
    "- \\( P(X_1 = 3|B) = \\frac{1}{5} \\) (1 out of 5 instances of class B)\n",
    "- \\( P(X_2 = 4|B) = \\frac{3}{5} \\) (3 out of 5 instances of class B)\n",
    "- Assuming independence, \\( P(X_1 = 3 \\cap X_2 = 4|B) = \\frac{1}{5} \\times \\frac{3}{5} = 0.12 \\)\n",
    "\n",
    "Given equal prior probabilities for both classes, we can see that the likelihood of the given instance with features \\( X_1 = 3 \\) and \\( X_2 = 4 \\) is the same for both class A and class B (0.12). Thus, Naive Bayes would not have a preference for class A or B based solely on these feature values. In cases like this, the classifier may fall back on a default decision rule (like always picking the first class), or use additional information to break the tie. If prior probabilities or additional evidence were considered, it could impact the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33de9b66-4e85-4fef-9038-c6e731f1812e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
